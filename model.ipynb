{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import FastText\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle \n",
    "train = pd.read_table('train.txt', header = None, names = ['text1','text2','label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1573 1730 8392 0 689 7 2702</td>\n",
       "      <td>96 1573 1730 8392</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4076 1223 3956 8784 1323</td>\n",
       "      <td>4076 8784 1323 1223 3956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1035 2149 6205 270 2646 1882 2689</td>\n",
       "      <td>3333 3009 17 1035 2149 6205 270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1039 31 0 12870 1743 2517 57 260</td>\n",
       "      <td>1039 31 0 12870 177 1743 2517 260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3500 1076 3865 22 3392 16096 5084</td>\n",
       "      <td>3500 1076 3865 2316 3392 225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text1                              text2  label\n",
       "0        1573 1730 8392 0 689 7 2702                  96 1573 1730 8392      1\n",
       "1           4076 1223 3956 8784 1323           4076 8784 1323 1223 3956      1\n",
       "2  1035 2149 6205 270 2646 1882 2689    3333 3009 17 1035 2149 6205 270      0\n",
       "3   1039 31 0 12870 1743 2517 57 260  1039 31 0 12870 177 1743 2517 260      1\n",
       "4  3500 1076 3865 22 3392 16096 5084       3500 1076 3865 2316 3392 225      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104771\n",
      "143229\n"
     ]
    }
   ],
   "source": [
    "#查看类别数\n",
    "print(sum(train['label']==0))\n",
    "print(sum(train['label']==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(text):\n",
    "    word_list = list(text.split())\n",
    "    return word_list\n",
    "train['text1'] = train[\"text1\"].apply(transform) \n",
    "train['text2'] = train[\"text2\"].apply(transform) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#观察句子长度\n",
    "length1 = [len(text) for text in train['text1']]\n",
    "length2 = [len(text) for text in train['text2']]\n",
    "plt.hist(length1)\n",
    "plt.show()\n",
    "plt.hist(length2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14119\n"
     ]
    }
   ],
   "source": [
    "##构建词典\n",
    "\n",
    "def preprocessing_word(s1_train, s2_train):\n",
    "    s1_all = []\n",
    "    s2_all = []\n",
    "    all_data = []\n",
    "\n",
    "    for s1,s2 in zip(s1_train, s2_train):\n",
    "\n",
    "        all_data.extend(s1)\n",
    "        all_data.extend(s2)\n",
    "        s1_all.append(s1)\n",
    "        s2_all.append(s2)\n",
    "\n",
    "    source_list = []\n",
    "    source_list.extend(list(set(all_data)))#生成词典\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    for index, char in enumerate(source_list):\n",
    "        word2id[char] = index\n",
    "        id2word[index] = char\n",
    "\n",
    "    return s1_all, s2_all, word2id, id2word\n",
    "\n",
    "s1_data = train['text1']\n",
    "s2_data = train['text2']\n",
    "s1_all,s2_all,word2id,id2word = preprocessing_word(s1_data,s2_data)\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>label</th>\n",
       "      <th>text1_after</th>\n",
       "      <th>text2_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1573, 1730, 8392, 0, 689, 7, 2702]</td>\n",
       "      <td>[96, 1573, 1730, 8392]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1573, 1730, 8392, 0, 689, 7, 2702]</td>\n",
       "      <td>[96, 1573, 1730, 8392]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4076, 1223, 3956, 8784, 1323]</td>\n",
       "      <td>[4076, 8784, 1323, 1223, 3956]</td>\n",
       "      <td>1</td>\n",
       "      <td>[4076, 1223, 3956, 8784, 1323]</td>\n",
       "      <td>[4076, 8784, 1323, 1223, 3956]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1035, 2149, 6205, 270, 2646, 1882, 2689]</td>\n",
       "      <td>[3333, 3009, 17, 1035, 2149, 6205, 270]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1035, 2149, 6205, 270, 2646, 1882, 2689]</td>\n",
       "      <td>[3333, 3009, 17, 1035, 2149, 6205, 270]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1039, 31, 0, 12870, 1743, 2517, 57, 260]</td>\n",
       "      <td>[1039, 31, 0, 12870, 177, 1743, 2517, 260]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1039, 31, 0, 12870, 1743, 2517, 57, 260]</td>\n",
       "      <td>[1039, 31, 0, 12870, 177, 1743, 2517, 260]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3500, 1076, 3865, 22, 3392, 16096, 5084]</td>\n",
       "      <td>[3500, 1076, 3865, 2316, 3392, 225]</td>\n",
       "      <td>0</td>\n",
       "      <td>[3500, 1076, 3865, 22, 3392, 16096, 5084]</td>\n",
       "      <td>[3500, 1076, 3865, 2316, 3392, 225]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text1  \\\n",
       "0        [1573, 1730, 8392, 0, 689, 7, 2702]   \n",
       "1             [4076, 1223, 3956, 8784, 1323]   \n",
       "2  [1035, 2149, 6205, 270, 2646, 1882, 2689]   \n",
       "3  [1039, 31, 0, 12870, 1743, 2517, 57, 260]   \n",
       "4  [3500, 1076, 3865, 22, 3392, 16096, 5084]   \n",
       "\n",
       "                                        text2  label  \\\n",
       "0                      [96, 1573, 1730, 8392]      1   \n",
       "1              [4076, 8784, 1323, 1223, 3956]      1   \n",
       "2     [3333, 3009, 17, 1035, 2149, 6205, 270]      0   \n",
       "3  [1039, 31, 0, 12870, 177, 1743, 2517, 260]      1   \n",
       "4         [3500, 1076, 3865, 2316, 3392, 225]      0   \n",
       "\n",
       "                                 text1_after  \\\n",
       "0        [1573, 1730, 8392, 0, 689, 7, 2702]   \n",
       "1             [4076, 1223, 3956, 8784, 1323]   \n",
       "2  [1035, 2149, 6205, 270, 2646, 1882, 2689]   \n",
       "3  [1039, 31, 0, 12870, 1743, 2517, 57, 260]   \n",
       "4  [3500, 1076, 3865, 22, 3392, 16096, 5084]   \n",
       "\n",
       "                                  text2_after  \n",
       "0                      [96, 1573, 1730, 8392]  \n",
       "1              [4076, 8784, 1323, 1223, 3956]  \n",
       "2     [3333, 3009, 17, 1035, 2149, 6205, 270]  \n",
       "3  [1039, 31, 0, 12870, 177, 1743, 2517, 260]  \n",
       "4         [3500, 1076, 3865, 2316, 3392, 225]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"text1_after\"], train[\"text2_after\"] = s1_all,s2_all\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##padding 和映射到固定长度\n",
    "\n",
    "max_sentence_length = 15\n",
    "def transform_word2id(data,word2id):\n",
    "    wordidlist = []\n",
    "    for i in data:\n",
    "        if i in word2id.keys():\n",
    "            wordidlist.append(word2id[i])\n",
    "        else:\n",
    "            wordidlist.append(word2id['0'])\n",
    "    if len(wordidlist) <= max_sentence_length:\n",
    "        wordidlist.extend([0]*(max_sentence_length-len(wordidlist)))\n",
    "        return wordidlist\n",
    "    else:\n",
    "        return wordidlist[:max_sentence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>label</th>\n",
       "      <th>text1_after</th>\n",
       "      <th>text2_after</th>\n",
       "      <th>text1_after_map</th>\n",
       "      <th>text2_after_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1573, 1730, 8392, 0, 689, 7, 2702]</td>\n",
       "      <td>[96, 1573, 1730, 8392]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1573, 1730, 8392, 0, 689, 7, 2702]</td>\n",
       "      <td>[96, 1573, 1730, 8392]</td>\n",
       "      <td>[14103, 3663, 5200, 7623, 7965, 3097, 12680, 0...</td>\n",
       "      <td>[3694, 14103, 3663, 5200, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4076, 1223, 3956, 8784, 1323]</td>\n",
       "      <td>[4076, 8784, 1323, 1223, 3956]</td>\n",
       "      <td>1</td>\n",
       "      <td>[4076, 1223, 3956, 8784, 1323]</td>\n",
       "      <td>[4076, 8784, 1323, 1223, 3956]</td>\n",
       "      <td>[11052, 8438, 10534, 3876, 11036, 0, 0, 0, 0, ...</td>\n",
       "      <td>[11052, 3876, 11036, 8438, 10534, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1035, 2149, 6205, 270, 2646, 1882, 2689]</td>\n",
       "      <td>[3333, 3009, 17, 1035, 2149, 6205, 270]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1035, 2149, 6205, 270, 2646, 1882, 2689]</td>\n",
       "      <td>[3333, 3009, 17, 1035, 2149, 6205, 270]</td>\n",
       "      <td>[4458, 278, 1296, 13057, 8052, 12826, 5601, 0,...</td>\n",
       "      <td>[7427, 1845, 8840, 4458, 278, 1296, 13057, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1039, 31, 0, 12870, 1743, 2517, 57, 260]</td>\n",
       "      <td>[1039, 31, 0, 12870, 177, 1743, 2517, 260]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1039, 31, 0, 12870, 1743, 2517, 57, 260]</td>\n",
       "      <td>[1039, 31, 0, 12870, 177, 1743, 2517, 260]</td>\n",
       "      <td>[1546, 11976, 7623, 13358, 1419, 6465, 13656, ...</td>\n",
       "      <td>[1546, 11976, 7623, 13358, 4934, 1419, 6465, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3500, 1076, 3865, 22, 3392, 16096, 5084]</td>\n",
       "      <td>[3500, 1076, 3865, 2316, 3392, 225]</td>\n",
       "      <td>0</td>\n",
       "      <td>[3500, 1076, 3865, 22, 3392, 16096, 5084]</td>\n",
       "      <td>[3500, 1076, 3865, 2316, 3392, 225]</td>\n",
       "      <td>[12035, 7217, 9200, 11097, 9932, 4241, 2620, 0...</td>\n",
       "      <td>[12035, 7217, 9200, 9848, 9932, 3314, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text1  \\\n",
       "0        [1573, 1730, 8392, 0, 689, 7, 2702]   \n",
       "1             [4076, 1223, 3956, 8784, 1323]   \n",
       "2  [1035, 2149, 6205, 270, 2646, 1882, 2689]   \n",
       "3  [1039, 31, 0, 12870, 1743, 2517, 57, 260]   \n",
       "4  [3500, 1076, 3865, 22, 3392, 16096, 5084]   \n",
       "\n",
       "                                        text2  label  \\\n",
       "0                      [96, 1573, 1730, 8392]      1   \n",
       "1              [4076, 8784, 1323, 1223, 3956]      1   \n",
       "2     [3333, 3009, 17, 1035, 2149, 6205, 270]      0   \n",
       "3  [1039, 31, 0, 12870, 177, 1743, 2517, 260]      1   \n",
       "4         [3500, 1076, 3865, 2316, 3392, 225]      0   \n",
       "\n",
       "                                 text1_after  \\\n",
       "0        [1573, 1730, 8392, 0, 689, 7, 2702]   \n",
       "1             [4076, 1223, 3956, 8784, 1323]   \n",
       "2  [1035, 2149, 6205, 270, 2646, 1882, 2689]   \n",
       "3  [1039, 31, 0, 12870, 1743, 2517, 57, 260]   \n",
       "4  [3500, 1076, 3865, 22, 3392, 16096, 5084]   \n",
       "\n",
       "                                  text2_after  \\\n",
       "0                      [96, 1573, 1730, 8392]   \n",
       "1              [4076, 8784, 1323, 1223, 3956]   \n",
       "2     [3333, 3009, 17, 1035, 2149, 6205, 270]   \n",
       "3  [1039, 31, 0, 12870, 177, 1743, 2517, 260]   \n",
       "4         [3500, 1076, 3865, 2316, 3392, 225]   \n",
       "\n",
       "                                     text1_after_map  \\\n",
       "0  [14103, 3663, 5200, 7623, 7965, 3097, 12680, 0...   \n",
       "1  [11052, 8438, 10534, 3876, 11036, 0, 0, 0, 0, ...   \n",
       "2  [4458, 278, 1296, 13057, 8052, 12826, 5601, 0,...   \n",
       "3  [1546, 11976, 7623, 13358, 1419, 6465, 13656, ...   \n",
       "4  [12035, 7217, 9200, 11097, 9932, 4241, 2620, 0...   \n",
       "\n",
       "                                     text2_after_map  \n",
       "0  [3694, 14103, 3663, 5200, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "1  [11052, 3876, 11036, 8438, 10534, 0, 0, 0, 0, ...  \n",
       "2  [7427, 1845, 8840, 4458, 278, 1296, 13057, 0, ...  \n",
       "3  [1546, 11976, 7623, 13358, 4934, 1419, 6465, 1...  \n",
       "4  [12035, 7217, 9200, 9848, 9932, 3314, 0, 0, 0,...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"text1_after_map\"] = train[\"text1_after\"].apply(transform_word2id,word2id = word2id)\n",
    "train[\"text2_after_map\"] = train[\"text2_after\"].apply(transform_word2id,word2id = word2id)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据存到一个大列表里面，格式是[[s1,s2,y],[s1,s2,y],[s1,s2,y].......]\n",
    "train_data = []\n",
    "for i in range(len(train[\"text1\"])):\n",
    "    train_data.append([train[\"text1_after_map\"][i],train[\"text2_after_map\"][i],train[\"label\"][i]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##保存词典\n",
    "# 将数据存入pickle中\n",
    "with open(\"word_data.pk\", 'wb') as f1:\n",
    "    pickle.dump((train_data, word2id,id2word), f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pk = 'word_data.pk'\n",
    "with open(data_pk, 'rb') as f:\n",
    "    train_data, word2id, id2word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置参数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "max_sentence_length = 15\n",
    "lr = 0.0001\n",
    "epoch_num = 20\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入embedding矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#从预训练模型中导入embedding矩阵\n",
    "def get_embedding_matrix(w2v, embedding_dim, isWV = False):\n",
    "    m = np.zeros(shape=(len(id2word), embedding_dim ))\n",
    "    for i, w in id2word.items():\n",
    "        if isWV:\n",
    "            if w not in w2v.wv.vocab.keys():\n",
    "                w2v.build_vocab(w, update=True)\n",
    "                w2v.train(w, total_examples=w2v.corpus_count, epochs=w2v.iter)\n",
    "            else:\n",
    "                m[i, :] = w2v[w]\n",
    "        else:\n",
    "            m[i, :] = w2v[w]\n",
    "    return m\n",
    "\n",
    "w2v = FastText.load('pretraining/fastText.model')\n",
    "embeddings = get_embedding_matrix(w2v, embedding_dim, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14119, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape # 权重矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据pad，生成batch数据返回，这里没有取余数。 切数据，返回n/batch_size个块\n",
    "def get_batch(data, batch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    :param data:\n",
    "    :param batch_size:\n",
    "    :param shuffle:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 乱序没有加\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data)\n",
    "    for i in range(len(data) // batch_size):\n",
    "        data_size = data[i * batch_size: (i + 1) * batch_size]\n",
    "        s1_data, s2_data, label_data = [], [], []\n",
    "        for (s1_set, s2_set, y_set) in data_size:\n",
    "            s1_data.append(s1_set)\n",
    "            s2_data.append(s2_set)\n",
    "            label_data.append(y_set)\n",
    "        yield np.array(s1_data), np.array(s2_data), np.reshape(label_data,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-8-a5d50e99acda>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-8-a5d50e99acda>:40: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-8-a5d50e99acda>:44: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From <ipython-input-8-a5d50e99acda>:61: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From <ipython-input-8-a5d50e99acda>:93: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\confusion_matrix.py:193: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "graph_mhd = tf.Graph()\n",
    "with graph_mhd.as_default():\n",
    "    \"\"\"\n",
    "    构建神经网络的结构、损失、优化方法和评估方法\n",
    "    \"\"\"\n",
    "    # shape[batch_size, 15]\n",
    "    left_input = tf.placeholder(tf.int64, shape=[None, max_sentence_length], name=\"left_input\")\n",
    "    # shape[batch_size, 15]\n",
    "    right_input = tf.placeholder(tf.int64, shape=[None, max_sentence_length], name=\"right_input\")\n",
    "\n",
    "    # shape[batch_size, labels]\n",
    "    labels = tf.placeholder(tf.float32, shape=[None, 1], name=\"labels\")\n",
    "    # labels维度要与输入维度一致\n",
    "    # [[1],[0],[1],[0],....]\n",
    "\n",
    "    # dropout = 1- keep_prob\n",
    "    dropout_pl = tf.placeholder(dtype=tf.float32, shape=(), name=\"dropout\")\n",
    "    # 0.2 \n",
    "    # x x x  \n",
    "    #  x x \n",
    "    # x x  x\n",
    "\n",
    "    with tf.variable_scope(\"embeddings\"):  # 命名空间\n",
    "        # Variable，网络中真实的变量\n",
    "        _word_embeddings = tf.Variable(embeddings,  # shape[len_words,200]\n",
    "                                       dtype=tf.float32,\n",
    "                                       trainable=True,  # 嵌入层是否可以训练 embeddings \n",
    "                                       name=\"embedding_matrix\")\n",
    "        # lookup，来一个id找对应的表示\n",
    "        left_embeddings = tf.nn.embedding_lookup(params=_word_embeddings, ids=left_input, name=\"left_embeddings\")\n",
    "        # left_embeddings_shape：[batchsize,15,200] , 每个id变为200维的表示\n",
    "        right_embeddings = tf.nn.embedding_lookup(params=_word_embeddings, ids=right_input, name=\"right_embeddings\")\n",
    "\n",
    "        left_embeddings = tf.nn.dropout(left_embeddings, dropout_pl) # embedding lookup之后加dropout\n",
    "        right_embeddings = tf.nn.dropout(right_embeddings, dropout_pl)\n",
    "\n",
    "    with tf.variable_scope(\"one_layer_bi-lstm\"):\n",
    "        # 词1层bi-lstm\n",
    "        # two-layers_bi-lstm\n",
    "        cell_fw = tf.nn.rnn_cell.LSTMCell(num_units=50)\n",
    "        cell_bw = tf.nn.rnn_cell.LSTMCell(num_units=50)\n",
    "        (left_output_fw_seq, left_output_bw_seq), left_states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw,\n",
    "                                                                                                left_embeddings,\n",
    "                                                                                                dtype=tf.float32)\n",
    "        # left_states_shape:[batchsize,100]\n",
    "        left_bi_output = tf.concat([left_states[0].h, left_states[1].h], axis=-1)\n",
    "\n",
    "        (right_output_fw_seq, right_output_bw_seq), right_states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw,\n",
    "                                                                                                   right_embeddings,\n",
    "                                                                                                   dtype=tf.float32)\n",
    "        right_bi_output = tf.concat([right_states[0].h, right_states[1].h], axis=-1) # 轴的操作\n",
    "        # -< []\n",
    "        # ->[]\n",
    "        # item\n",
    "\n",
    "    with tf.variable_scope(\"Similarity_calculation_layer\"):\n",
    "        def cosine_dist(input1,input2):\n",
    "            pooled_len_1 = tf.sqrt(tf.reduce_sum(input1 * input1, 1))\n",
    "            pooled_len_2 = tf.sqrt(tf.reduce_sum(input2 * input2, 1))\n",
    "            pooled_mul_12 = tf.reduce_sum(input1 * input2, 1)\n",
    "            score = tf.div(pooled_mul_12, pooled_len_1 * pooled_len_2 +1e-8, name=\"scores\")\n",
    "            return score\n",
    "\n",
    "        def manhattan_dist(input1,input2):\n",
    "            score = tf.exp(-tf.reduce_sum(tf.abs(input1-input2), 1))\n",
    "            return score\n",
    "        def multiply(input1,input2):\n",
    "            score = tf.multiply(input1, input2)  # 矩阵按元素相乘\n",
    "            #tf.matmul(matrix3, matrix2)  # 矩阵点积\n",
    "            return score\n",
    "        def subtract(input1,input2):\n",
    "            score = tf.abs(input1-input2)\n",
    "            return score\n",
    "        def maximum(input1,input2):\n",
    "            s1 = multiply(input1,input1)\n",
    "            s2 = multiply(input2,input2)\n",
    "            score = tf.maximum(s1,s2)\n",
    "            return score\n",
    "\n",
    "        cos = cosine_dist(left_bi_output, right_bi_output)\n",
    "        # [batch_size,]\n",
    "        man = manhattan_dist(left_bi_output, right_bi_output)\n",
    "        mul = multiply(left_bi_output, right_bi_output)\n",
    "        # [batch_size,100]\n",
    "        sub = subtract(left_bi_output, right_bi_output)\n",
    "        maxium = maximum(left_bi_output, right_bi_output)\n",
    "        # 曼哈顿距离\n",
    "        # output = tf.expand_dims(manhattan_dist(left_bi_output, right_bi_output), -1)\n",
    "        # 相似度计算理解清楚？？？？\n",
    "        last_list_layer = tf.concat([mul, sub, maxium], 1)\n",
    "        last_drop = tf.nn.dropout(last_list_layer, 0.8)\n",
    "        # 降维，300 -> 16\n",
    "        dense_layer1 = tf.layers.dense(last_drop, 16, activation=tf.nn.relu)\n",
    "        dense_layer2 = tf.layers.dense(last_drop, 24, activation=tf.nn.sigmoid)\n",
    "        # expand_dims 升维\n",
    "        output = tf.concat([dense_layer1, dense_layer2, tf.expand_dims(cos, -1), tf.expand_dims(man, -1)], 1)\n",
    "\n",
    "\n",
    "    with tf.variable_scope(\"classification\"):\n",
    "        # logits:shape[batch_size, labels]\n",
    "        # 层数越多，效果越好\n",
    "        # 残差结构\n",
    "        # dense 全连接层\n",
    "        output = tf.layers.dense(output, 32)\n",
    "        logits = tf.layers.dense(output, 1)\n",
    "\n",
    "    # 计算损失\n",
    "    # 计算分布之间的距离\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels,logits=logits))\n",
    "\n",
    "\n",
    "\n",
    "    #选择优化器\n",
    "    # 优化器的发展历程\n",
    "    # BGD、SGD、momentum\n",
    "    # 了解过程\n",
    "    with tf.variable_scope(\"train_step\"):\n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "    # 准确率/f1/p/r计算\n",
    "    with tf.variable_scope(\"evaluation\"):\n",
    "        pred_rate = tf.sigmoid(logits,name=\"sig\")\n",
    "        pred = tf.cast(tf.greater(pred_rate, 0.5), tf.float32)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, labels), tf.float32), name=\"accuracy\")\n",
    "        # 混淆矩阵\n",
    "        # _|0 |1 |\n",
    "        # 0|2 |3 |\n",
    "        # 1|2 |3 |\n",
    "\n",
    "        true = tf.reshape(labels, (-1,))\n",
    "        pred = tf.reshape(pred, (-1,))\n",
    "\n",
    "        epsilon = 1e-7\n",
    "        cm = tf.contrib.metrics.confusion_matrix(true, pred, num_classes=2)\n",
    "\n",
    "        precision = tf.cast(cm[1][1] / tf.reduce_sum(cm[:,1]),tf.float32,name=\"precision\")\n",
    "        recall = tf.cast(cm[1][1] / tf.reduce_sum(cm[1],axis=0),tf.float32,name=\"recall\")\n",
    "        f1_score = tf.cast((2 * precision * recall / (precision + recall + epsilon)),tf.float32,name=\"f1_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 15 399]\n",
      " [  6 580]]\n",
      "train: epoch 0, global_step 50, loss: 0.6723, accuracy: 0.595 , precision 0.5924412608146667, recall: 0.9898, fbeta_score: 0.7412 \n",
      "[[ 37 375]\n",
      " [ 10 578]]\n",
      "train: epoch 0, global_step 100, loss: 0.6602, accuracy: 0.615 , precision 0.6065057516098022, recall: 0.983, fbeta_score: 0.7502 \n",
      "[[110 281]\n",
      " [ 48 561]]\n",
      "train: epoch 0, global_step 150, loss: 0.6242, accuracy: 0.671 , precision 0.6662707924842834, recall: 0.9212, fbeta_score: 0.7733 \n",
      "[[184 242]\n",
      " [ 70 504]]\n",
      "train: epoch 0, global_step 200, loss: 0.6178, accuracy: 0.688 , precision 0.6756032109260559, recall: 0.878, fbeta_score: 0.7636 \n",
      "-----------------validation---------------\n",
      "[[2017 3228]\n",
      " [ 871 6284]]\n",
      "validation: epoch 0, global_step 200, loss: 0.6173, accuracy: 0.6694 , precision 0.6606391668319702, recall: 0.8783, fbeta_score: 0.7541 \n",
      "-----------------validation---------------\n",
      "[[177 247]\n",
      " [ 83 493]]\n",
      "train: epoch 1, global_step 250, loss: 0.6185, accuracy: 0.67 , precision 0.6662161946296692, recall: 0.8559, fbeta_score: 0.7492 \n",
      "[[199 243]\n",
      " [ 71 487]]\n",
      "train: epoch 1, global_step 300, loss: 0.5919, accuracy: 0.686 , precision 0.6671232581138611, recall: 0.8728, fbeta_score: 0.7562 \n",
      "[[248 176]\n",
      " [ 88 488]]\n",
      "train: epoch 1, global_step 350, loss: 0.5537, accuracy: 0.736 , precision 0.7349397540092468, recall: 0.8472, fbeta_score: 0.7871 \n",
      "[[252 169]\n",
      " [ 99 480]]\n",
      "train: epoch 1, global_step 400, loss: 0.5339, accuracy: 0.732 , precision 0.7395994067192078, recall: 0.829, fbeta_score: 0.7818 \n",
      "-----------------validation---------------\n",
      "[[2937 2246]\n",
      " [1014 6203]]\n",
      "validation: epoch 1, global_step 400, loss: 0.5342, accuracy: 0.7371 , precision 0.7341697216033936, recall: 0.8595, fbeta_score: 0.7919 \n",
      "-----------------validation---------------\n",
      "[[287 132]\n",
      " [107 474]]\n",
      "train: epoch 1, global_step 450, loss: 0.5216, accuracy: 0.761 , precision 0.7821782231330872, recall: 0.8158, fbeta_score: 0.7987 \n",
      "[[260 156]\n",
      " [ 79 505]]\n",
      "train: epoch 2, global_step 500, loss: 0.4887, accuracy: 0.765 , precision 0.7639939188957214, recall: 0.8647, fbeta_score: 0.8112 \n",
      "[[310 142]\n",
      " [ 82 466]]\n",
      "train: epoch 2, global_step 550, loss: 0.4903, accuracy: 0.776 , precision 0.7664473652839661, recall: 0.8504, fbeta_score: 0.8062 \n",
      "[[295 122]\n",
      " [ 72 511]]\n",
      "train: epoch 2, global_step 600, loss: 0.4589, accuracy: 0.806 , precision 0.8072670102119446, recall: 0.8765, fbeta_score: 0.8405 \n",
      "-----------------validation---------------\n",
      "[[3435 1811]\n",
      " [ 942 6212]]\n",
      "validation: epoch 2, global_step 600, loss: 0.4698, accuracy: 0.778 , precision 0.7742739915847778, recall: 0.8683, fbeta_score: 0.8186 \n",
      "-----------------validation---------------\n",
      "[[285 131]\n",
      " [ 84 500]]\n",
      "train: epoch 2, global_step 650, loss: 0.4664, accuracy: 0.785 , precision 0.7923930287361145, recall: 0.8562, fbeta_score: 0.823 \n",
      "[[255 142]\n",
      " [ 71 532]]\n",
      "train: epoch 2, global_step 700, loss: 0.4535, accuracy: 0.787 , precision 0.7893174886703491, recall: 0.8823, fbeta_score: 0.8332 \n",
      "[[285 151]\n",
      " [ 87 477]]\n",
      "train: epoch 3, global_step 750, loss: 0.4582, accuracy: 0.762 , precision 0.7595541477203369, recall: 0.8457, fbeta_score: 0.8003 \n",
      "[[317 110]\n",
      " [ 73 500]]\n",
      "train: epoch 3, global_step 800, loss: 0.4173, accuracy: 0.817 , precision 0.8196721076965332, recall: 0.8726, fbeta_score: 0.8453 \n",
      "-----------------validation---------------\n",
      "[[3634 1587]\n",
      " [ 832 6347]]\n",
      "validation: epoch 3, global_step 800, loss: 0.426, accuracy: 0.8049 , precision 0.799974799156189, recall: 0.8841, fbeta_score: 0.8399 \n",
      "-----------------validation---------------\n",
      "[[301 114]\n",
      " [ 95 490]]\n",
      "train: epoch 3, global_step 850, loss: 0.4645, accuracy: 0.791 , precision 0.8112582564353943, recall: 0.8376, fbeta_score: 0.8242 \n",
      "[[306 103]\n",
      " [ 88 503]]\n",
      "train: epoch 3, global_step 900, loss: 0.408, accuracy: 0.809 , precision 0.830033004283905, recall: 0.8511, fbeta_score: 0.8404 \n",
      "[[319 131]\n",
      " [ 64 486]]\n",
      "train: epoch 4, global_step 950, loss: 0.4205, accuracy: 0.805 , precision 0.7876823544502258, recall: 0.8836, fbeta_score: 0.8329 \n",
      "[[325 104]\n",
      " [ 73 498]]\n",
      "train: epoch 4, global_step 1000, loss: 0.4093, accuracy: 0.823 , precision 0.8272425532341003, recall: 0.8722, fbeta_score: 0.8491 \n",
      "-----------------validation---------------\n",
      "[[3819 1507]\n",
      " [ 711 6363]]\n",
      "validation: epoch 4, global_step 1000, loss: 0.406, accuracy: 0.8211 , precision 0.808513343334198, recall: 0.8995, fbeta_score: 0.8516 \n",
      "-----------------validation---------------\n",
      "[[307 129]\n",
      " [ 69 495]]\n",
      "train: epoch 4, global_step 1050, loss: 0.4459, accuracy: 0.802 , precision 0.7932692170143127, recall: 0.8777, fbeta_score: 0.8333 \n",
      "[[298 122]\n",
      " [ 82 498]]\n",
      "train: epoch 4, global_step 1100, loss: 0.4307, accuracy: 0.796 , precision 0.8032258152961731, recall: 0.8586, fbeta_score: 0.83 \n",
      "[[300 119]\n",
      " [ 75 506]]\n",
      "train: epoch 4, global_step 1150, loss: 0.4348, accuracy: 0.806 , precision 0.8095999956130981, recall: 0.8709, fbeta_score: 0.8391 \n",
      "[[330 106]\n",
      " [ 78 486]]\n",
      "train: epoch 5, global_step 1200, loss: 0.4274, accuracy: 0.816 , precision 0.8209459185600281, recall: 0.8617, fbeta_score: 0.8408 \n",
      "-----------------validation---------------\n",
      "[[3844 1333]\n",
      " [ 785 6438]]\n",
      "validation: epoch 5, global_step 1200, loss: 0.3824, accuracy: 0.8292 , precision 0.8284648060798645, recall: 0.8913, fbeta_score: 0.8587 \n",
      "-----------------validation---------------\n",
      "[[324  86]\n",
      " [ 82 508]]\n",
      "train: epoch 5, global_step 1250, loss: 0.3923, accuracy: 0.832 , precision 0.8552188277244568, recall: 0.861, fbeta_score: 0.8581 \n",
      "[[343 114]\n",
      " [ 68 475]]\n",
      "train: epoch 5, global_step 1300, loss: 0.4093, accuracy: 0.818 , precision 0.8064516186714172, recall: 0.8748, fbeta_score: 0.8392 \n",
      "[[340  88]\n",
      " [ 70 502]]\n",
      "train: epoch 5, global_step 1350, loss: 0.371, accuracy: 0.842 , precision 0.8508474826812744, recall: 0.8776, fbeta_score: 0.864 \n",
      "[[283  99]\n",
      " [ 88 530]]\n",
      "train: epoch 5, global_step 1400, loss: 0.4027, accuracy: 0.813 , precision 0.842607319355011, recall: 0.8576, fbeta_score: 0.85 \n",
      "-----------------validation---------------\n",
      "[[3802 1375]\n",
      " [ 734 6489]]\n",
      "validation: epoch 5, global_step 1400, loss: 0.381, accuracy: 0.8299 , precision 0.825152575969696, recall: 0.8984, fbeta_score: 0.8602 \n",
      "-----------------validation---------------\n",
      "[[329  99]\n",
      " [ 70 502]]\n",
      "train: epoch 6, global_step 1450, loss: 0.3897, accuracy: 0.831 , precision 0.8352745175361633, recall: 0.8776, fbeta_score: 0.8559 \n",
      "[[334  77]\n",
      " [ 75 514]]\n",
      "train: epoch 6, global_step 1500, loss: 0.3432, accuracy: 0.848 , precision 0.8697123527526855, recall: 0.8727, fbeta_score: 0.8712 \n",
      "[[337  90]\n",
      " [ 68 505]]\n",
      "train: epoch 6, global_step 1550, loss: 0.3652, accuracy: 0.842 , precision 0.848739504814148, recall: 0.8813, fbeta_score: 0.8647 \n",
      "[[295  93]\n",
      " [ 67 545]]\n",
      "train: epoch 6, global_step 1600, loss: 0.3841, accuracy: 0.84 , precision 0.8542319536209106, recall: 0.8905, fbeta_score: 0.872 \n",
      "-----------------validation---------------\n",
      "[[3987 1193]\n",
      " [ 708 6512]]\n",
      "validation: epoch 6, global_step 1600, loss: 0.3501, accuracy: 0.8467 , precision 0.845165491104126, recall: 0.9019, fbeta_score: 0.8726 \n",
      "-----------------validation---------------\n",
      "[[350  84]\n",
      " [ 78 488]]\n",
      "train: epoch 7, global_step 1650, loss: 0.3547, accuracy: 0.838 , precision 0.8531468510627747, recall: 0.8622, fbeta_score: 0.8576 \n",
      "[[369 113]\n",
      " [ 57 461]]\n",
      "train: epoch 7, global_step 1700, loss: 0.3824, accuracy: 0.83 , precision 0.803135871887207, recall: 0.89, fbeta_score: 0.8443 \n",
      "[[318  73]\n",
      " [ 77 532]]\n",
      "train: epoch 7, global_step 1750, loss: 0.376, accuracy: 0.85 , precision 0.8793388605117798, recall: 0.8736, fbeta_score: 0.8764 \n",
      "[[343  93]\n",
      " [ 60 504]]\n",
      "train: epoch 7, global_step 1800, loss: 0.3428, accuracy: 0.847 , precision 0.8442211151123047, recall: 0.8936, fbeta_score: 0.8682 \n",
      "-----------------validation---------------\n",
      "[[4003 1194]\n",
      " [ 650 6553]]\n",
      "validation: epoch 7, global_step 1800, loss: 0.3427, accuracy: 0.8513 , precision 0.8458757996559143, recall: 0.9098, fbeta_score: 0.8767 \n",
      "-----------------validation---------------\n",
      "[[317  92]\n",
      " [ 62 529]]\n",
      "train: epoch 7, global_step 1850, loss: 0.3502, accuracy: 0.846 , precision 0.8518518805503845, recall: 0.8951, fbeta_score: 0.8729 \n",
      "[[355  67]\n",
      " [ 65 513]]\n",
      "train: epoch 8, global_step 1900, loss: 0.339, accuracy: 0.868 , precision 0.884482741355896, recall: 0.8875, fbeta_score: 0.886 \n",
      "[[352  83]\n",
      " [ 73 492]]\n",
      "train: epoch 8, global_step 1950, loss: 0.3837, accuracy: 0.844 , precision 0.8556521534919739, recall: 0.8708, fbeta_score: 0.8632 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[330  70]\n",
      " [ 65 535]]\n",
      "train: epoch 8, global_step 2000, loss: 0.3243, accuracy: 0.865 , precision 0.8842975497245789, recall: 0.8917, fbeta_score: 0.888 \n",
      "-----------------validation---------------\n",
      "[[4145 1035]\n",
      " [ 747 6473]]\n",
      "validation: epoch 8, global_step 2000, loss: 0.3273, accuracy: 0.8563 , precision 0.8621470332145691, recall: 0.8965, fbeta_score: 0.879 \n",
      "-----------------validation---------------\n",
      "[[330  84]\n",
      " [ 58 528]]\n",
      "train: epoch 8, global_step 2050, loss: 0.342, accuracy: 0.858 , precision 0.8627451062202454, recall: 0.901, fbeta_score: 0.8815 \n",
      "[[328  89]\n",
      " [ 75 508]]\n",
      "train: epoch 8, global_step 2100, loss: 0.3762, accuracy: 0.836 , precision 0.8509212732315063, recall: 0.8714, fbeta_score: 0.861 \n",
      "[[348  73]\n",
      " [ 74 505]]\n",
      "train: epoch 9, global_step 2150, loss: 0.3286, accuracy: 0.853 , precision 0.8737024068832397, recall: 0.8722, fbeta_score: 0.8729 \n",
      "[[334  87]\n",
      " [ 67 512]]\n",
      "train: epoch 9, global_step 2200, loss: 0.3587, accuracy: 0.846 , precision 0.8547579050064087, recall: 0.8843, fbeta_score: 0.8693 \n",
      "-----------------validation---------------\n",
      "[[4219 1005]\n",
      " [ 661 6515]]\n",
      "validation: epoch 9, global_step 2200, loss: 0.3186, accuracy: 0.8656 , precision 0.866356372833252, recall: 0.9079, fbeta_score: 0.8866 \n",
      "-----------------validation---------------\n",
      "[[346  93]\n",
      " [ 63 498]]\n",
      "train: epoch 9, global_step 2250, loss: 0.376, accuracy: 0.844 , precision 0.8426395654678345, recall: 0.8877, fbeta_score: 0.8646 \n",
      "[[338  88]\n",
      " [ 81 493]]\n",
      "train: epoch 9, global_step 2300, loss: 0.3724, accuracy: 0.831 , precision 0.848537027835846, recall: 0.8589, fbeta_score: 0.8537 \n",
      "[[345  83]\n",
      " [ 52 520]]\n",
      "train: epoch 9, global_step 2350, loss: 0.3364, accuracy: 0.865 , precision 0.8623548746109009, recall: 0.9091, fbeta_score: 0.8851 \n",
      "[[343  78]\n",
      " [ 54 525]]\n",
      "train: epoch 10, global_step 2400, loss: 0.3191, accuracy: 0.868 , precision 0.8706467747688293, recall: 0.9067, fbeta_score: 0.8883 \n",
      "-----------------validation---------------\n",
      "[[4122 1006]\n",
      " [ 646 6626]]\n",
      "validation: epoch 10, global_step 2400, loss: 0.3095, accuracy: 0.8668 , precision 0.8681865930557251, recall: 0.9112, fbeta_score: 0.8892 \n",
      "-----------------validation---------------\n",
      "[[341  84]\n",
      " [ 44 531]]\n",
      "train: epoch 10, global_step 2450, loss: 0.3144, accuracy: 0.872 , precision 0.8634146451950073, recall: 0.9235, fbeta_score: 0.8924 \n",
      "[[323  85]\n",
      " [ 67 525]]\n",
      "train: epoch 10, global_step 2500, loss: 0.3537, accuracy: 0.848 , precision 0.8606557250022888, recall: 0.8868, fbeta_score: 0.8735 \n",
      "[[348  80]\n",
      " [ 69 503]]\n",
      "train: epoch 10, global_step 2550, loss: 0.36, accuracy: 0.851 , precision 0.8627787232398987, recall: 0.8794, fbeta_score: 0.871 \n",
      "[[338  75]\n",
      " [ 50 537]]\n",
      "train: epoch 11, global_step 2600, loss: 0.2993, accuracy: 0.875 , precision 0.8774510025978088, recall: 0.9148, fbeta_score: 0.8957 \n",
      "-----------------validation---------------\n",
      "[[4312  968]\n",
      " [ 625 6495]]\n",
      "validation: epoch 11, global_step 2600, loss: 0.2982, accuracy: 0.8715 , precision 0.8702934384346008, recall: 0.9122, fbeta_score: 0.8908 \n",
      "-----------------validation---------------\n",
      "[[394  65]\n",
      " [ 66 475]]\n",
      "train: epoch 11, global_step 2650, loss: 0.3086, accuracy: 0.869 , precision 0.8796296119689941, recall: 0.878, fbeta_score: 0.8788 \n",
      "[[336  76]\n",
      " [ 68 520]]\n",
      "train: epoch 11, global_step 2700, loss: 0.3398, accuracy: 0.856 , precision 0.8724831938743591, recall: 0.8844, fbeta_score: 0.8784 \n",
      "[[355  61]\n",
      " [ 65 519]]\n",
      "train: epoch 11, global_step 2750, loss: 0.3091, accuracy: 0.874 , precision 0.8948276042938232, recall: 0.8887, fbeta_score: 0.8918 \n",
      "[[359  64]\n",
      " [ 54 523]]\n",
      "train: epoch 11, global_step 2800, loss: 0.3039, accuracy: 0.882 , precision 0.8909710645675659, recall: 0.9064, fbeta_score: 0.8986 \n",
      "-----------------validation---------------\n",
      "[[4331  949]\n",
      " [ 648 6472]]\n",
      "validation: epoch 11, global_step 2800, loss: 0.3028, accuracy: 0.8712 , precision 0.872119665145874, recall: 0.909, fbeta_score: 0.8902 \n",
      "-----------------validation---------------\n",
      "[[363  65]\n",
      " [ 63 509]]\n",
      "train: epoch 12, global_step 2850, loss: 0.3372, accuracy: 0.872 , precision 0.8867595791816711, recall: 0.8899, fbeta_score: 0.8883 \n",
      "[[348  86]\n",
      " [ 79 487]]\n",
      "train: epoch 12, global_step 2900, loss: 0.3642, accuracy: 0.835 , precision 0.8499127626419067, recall: 0.8604, fbeta_score: 0.8551 \n",
      "[[370  65]\n",
      " [ 58 507]]\n",
      "train: epoch 12, global_step 2950, loss: 0.3041, accuracy: 0.877 , precision 0.8863636255264282, recall: 0.8973, fbeta_score: 0.8918 \n",
      "[[361  72]\n",
      " [ 63 504]]\n",
      "train: epoch 12, global_step 3000, loss: 0.3165, accuracy: 0.865 , precision 0.875, recall: 0.8889, fbeta_score: 0.8819 \n",
      "-----------------validation---------------\n",
      "[[4361  890]\n",
      " [ 661 6488]]\n",
      "validation: epoch 12, global_step 3000, loss: 0.293, accuracy: 0.8749 , precision 0.8793711066246033, recall: 0.9075, fbeta_score: 0.8932 \n",
      "-----------------validation---------------\n",
      "[[367  78]\n",
      " [ 52 503]]\n",
      "train: epoch 12, global_step 3050, loss: 0.2944, accuracy: 0.87 , precision 0.8657487034797668, recall: 0.9063, fbeta_score: 0.8856 \n",
      "[[348  69]\n",
      " [ 58 525]]\n",
      "train: epoch 13, global_step 3100, loss: 0.3219, accuracy: 0.873 , precision 0.8838383555412292, recall: 0.9005, fbeta_score: 0.8921 \n",
      "[[375  79]\n",
      " [ 48 498]]\n",
      "train: epoch 13, global_step 3150, loss: 0.3133, accuracy: 0.873 , precision 0.8630849123001099, recall: 0.9121, fbeta_score: 0.8869 \n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_mhd) as sess:\n",
    "    if True:\n",
    "        # 保存\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        global_nums = 0\n",
    "        for epoch in range(epoch_num):\n",
    "            np.random.shuffle(train_data)\n",
    "            ratio = int(len(train_data)*0.95)\n",
    "            train= train_data[:ratio]\n",
    "            validation = train_data[ratio:]\n",
    "            for s1, s2, y in get_batch(train, batch_size, shuffle=True):\n",
    "                _, l, acc, p, r, f, cmm = sess.run(\n",
    "                    [train_op, loss,accuracy,precision,recall,f1_score,cm], {\n",
    "                    left_input: s1,\n",
    "                    right_input: s2,\n",
    "                    labels : y,\n",
    "                    dropout_pl: 0.8\n",
    "                })\n",
    "                global_nums += 1\n",
    "                if global_nums % 50 == 0:\n",
    "                    print(cmm)\n",
    "                    # saver.save(sess, '../model_save/model.ckpt', global_step=global_nums)\n",
    "                    print(\n",
    "                        'train: epoch {}, global_step {}, loss: {:.4}, accuracy: {:.4} , precision {}, recall: {:.4}, fbeta_score: {:.4} '.format(epoch , global_nums,\n",
    "                                                                                          l, acc,p, r, f))\n",
    "\n",
    "\n",
    "                if global_nums % 200 == 0:\n",
    "                    print('-----------------validation---------------')\n",
    "                    s1, s2, y = next(get_batch(validation, np.shape(validation)[0],  shuffle=True))\n",
    "                    l, acc, p, r, f,cmm = sess.run(\n",
    "                        [ loss, accuracy, precision, recall, f1_score,cm], {\n",
    "                            left_input: s1,\n",
    "                            right_input: s2,\n",
    "                            labels: y,\n",
    "                            dropout_pl: 1\n",
    "                        })\n",
    "                    print(cmm)\n",
    "                    print(\n",
    "                        'validation: epoch {}, global_step {}, loss: {:.4}, accuracy: {:.4} , precision {}, recall: {:.4}, fbeta_score: {:.4} '.format(\n",
    "                            epoch, global_nums,\n",
    "                            l, acc, p, r, f))\n",
    "                    print('-----------------validation---------------')\n",
    "        s1, s2, y = next(get_batch(validation, np.shape(validation)[0], shuffle=True))\n",
    "        l, acc, p, r, f, cmm = sess.run(\n",
    "                    [loss, accuracy, precision, recall, f1_score, cm], {\n",
    "                        left_input: s1,\n",
    "                        right_input: s2,\n",
    "                        labels: y,\n",
    "                        dropout_pl: 1\n",
    "                    })\n",
    "        print(cmm)\n",
    "        saver.save(sess, 'ckpt2/save_model', global_step=global_nums)\n",
    "        print('validation: loss: {:.4}, accuracy: {:.4} , precision {}, recall: {:.4}, fbeta_score: {:.4} '.format(\n",
    "                        l, acc, p, r, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
